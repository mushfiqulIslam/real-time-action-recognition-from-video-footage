{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dense-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from tools.settings import *\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tools.train_val_test_spliter import split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-summary",
   "metadata": {},
   "source": [
    "### Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fleet-richmond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the ginen dataset into Train Test=0.2 Validation=0.2\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# To split dataset if already splits folder already exits no need to run it\n",
    "split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "patient-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd. read_csv(os.path.join(dataset_path, \"train.csv\"))\n",
    "test = pd. read_csv(os.path.join(dataset_path, \"test.csv\"))\n",
    "val = pd. read_csv(os.path.join(dataset_path, \"val.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "persistent-anderson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video_url</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H:/THESIS/dataset\\kick/kick_192.mp4</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H:/THESIS/dataset\\kick/kick_197.mp4</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H:/THESIS/dataset\\slap/Slap_160.mp4</td>\n",
       "      <td>slap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H:/THESIS/dataset\\kick/kick_183.mp4</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H:/THESIS/dataset\\kick/kick_17.mp4</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Video_url action\n",
       "0  H:/THESIS/dataset\\kick/kick_192.mp4   kick\n",
       "1  H:/THESIS/dataset\\kick/kick_197.mp4   kick\n",
       "2  H:/THESIS/dataset\\slap/Slap_160.mp4   slap\n",
       "3  H:/THESIS/dataset\\kick/kick_183.mp4   kick\n",
       "4   H:/THESIS/dataset\\kick/kick_17.mp4   kick"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subtle-trustee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video_url</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H:/THESIS/dataset\\kick/kick_191.mp4</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H:/THESIS/dataset\\slap/Slap_158.mp4</td>\n",
       "      <td>slap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H:/THESIS/dataset\\kick/kick_162.mp4</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H:/THESIS/dataset\\punch/punch_128.mp4</td>\n",
       "      <td>punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H:/THESIS/dataset\\punch/punch_187.mp4</td>\n",
       "      <td>punch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Video_url action\n",
       "0    H:/THESIS/dataset\\kick/kick_191.mp4   kick\n",
       "1    H:/THESIS/dataset\\slap/Slap_158.mp4   slap\n",
       "2    H:/THESIS/dataset\\kick/kick_162.mp4   kick\n",
       "3  H:/THESIS/dataset\\punch/punch_128.mp4  punch\n",
       "4  H:/THESIS/dataset\\punch/punch_187.mp4  punch"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "entire-camel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video_url</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H:/THESIS/dataset\\slap/slap_136.mp4</td>\n",
       "      <td>slap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H:/THESIS/dataset\\kick/kick_187.mp4</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H:/THESIS/dataset\\punch/punch_172.mp4</td>\n",
       "      <td>punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H:/THESIS/dataset\\punch/punch_111.mp4</td>\n",
       "      <td>punch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H:/THESIS/dataset\\slap/slap_26.mp4</td>\n",
       "      <td>slap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Video_url action\n",
       "0    H:/THESIS/dataset\\slap/slap_136.mp4   slap\n",
       "1    H:/THESIS/dataset\\kick/kick_187.mp4   kick\n",
       "2  H:/THESIS/dataset\\punch/punch_172.mp4  punch\n",
       "3  H:/THESIS/dataset\\punch/punch_111.mp4  punch\n",
       "4     H:/THESIS/dataset\\slap/slap_26.mp4   slap"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "extended-carolina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(370, 2)\n",
      "(123, 2)\n",
      "(123, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fourth-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_frame(data, folder_name):\n",
    "    '''\n",
    "    Generated filenames format dataset_path/folder_name/video_name_frame{number}_action.jpg\n",
    "    '''\n",
    "    directory = os.path.join(dataset_path, folder_name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    for i in tqdm(range(data.shape[0])):\n",
    "        video_file = data['Video_url'][i]\n",
    "        action = data['action'][i]\n",
    "        video_name_list = video_file.split('/')[-1].split('.')\n",
    "        video_name_list = video_name_list[:-1]\n",
    "        video_name = \"\"\n",
    "        for n in video_name_list:\n",
    "            video_name += n\n",
    "        # capturing the video from the given path\n",
    "        capture = cv2.VideoCapture(video_file) \n",
    "        #frame rate\n",
    "        frame_rate = capture.get(5)\n",
    "        count = 0\n",
    "        while(capture.isOpened()):\n",
    "            #current frame number\n",
    "            frame_id = capture.get(1) \n",
    "            read_correctly, frame = capture.read()\n",
    "            if not read_correctly:\n",
    "                break\n",
    "            if (frame_id % math.floor(frame_rate) == 0):\n",
    "                # storing the frames in a new folder named train_1\n",
    "                filename = directory + \"/\" + video_name + \"_frame{}_\".format(count) + action +\".jpg\"\n",
    "                count += 1\n",
    "                cv2.imwrite(filename, frame)\n",
    "        capture.release()\n",
    "    print(\"Successfully Converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "double-corporation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 370/370 [03:47<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Converted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "convert_to_frame(train, train_frames_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "magnetic-roberts",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 123/123 [01:10<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully Converted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "convert_to_frame(val, val_frames_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "secondary-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paths_csv(directory, file_name):\n",
    "    images = os.listdir(directory)\n",
    "    images_path_list = []\n",
    "    images_action_list = [] \n",
    "    for image in images:\n",
    "        images_path_list.append(directory + image)\n",
    "        images_action_list.append(image.split('.')[0].split('_')[-1])\n",
    "    df = pd.DataFrame()\n",
    "    df['image'] = images_path_list\n",
    "    df['action'] = images_action_list\n",
    "    print(os.path.join(dataset_path, file_name+'.csv'))\n",
    "    df.to_csv(os.path.join(dataset_path, file_name+'.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "human-horizon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:/THESIS/dataset\\train_frames.csv\n"
     ]
    }
   ],
   "source": [
    "create_paths_csv(train_frames_path, train_frames_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "unlikely-treaty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:/THESIS/dataset\\val_frames.csv\n"
     ]
    }
   ],
   "source": [
    "create_paths_csv(val_frames_path, val_frames_path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "understood-cleaners",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H:/THESIS/dataset\\train_frames\\kick_01_frame0_...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H:/THESIS/dataset\\train_frames\\kick_01_frame1_...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H:/THESIS/dataset\\train_frames\\kick_01_frame2_...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H:/THESIS/dataset\\train_frames\\kick_01_frame3_...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H:/THESIS/dataset\\train_frames\\kick_02_frame0_...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image action\n",
       "0  H:/THESIS/dataset\\train_frames\\kick_01_frame0_...   kick\n",
       "1  H:/THESIS/dataset\\train_frames\\kick_01_frame1_...   kick\n",
       "2  H:/THESIS/dataset\\train_frames\\kick_01_frame2_...   kick\n",
       "3  H:/THESIS/dataset\\train_frames\\kick_01_frame3_...   kick\n",
       "4  H:/THESIS/dataset\\train_frames\\kick_02_frame0_...   kick"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image = pd.read_csv(os.path.join(dataset_path, 'train_frames.csv'))\n",
    "train_image.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "hazardous-extraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2171, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "trained-bread",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H:/THESIS/dataset\\val_frames\\kick_02_frame0_ki...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H:/THESIS/dataset\\val_frames\\kick_02_frame1_ki...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H:/THESIS/dataset\\val_frames\\kick_07_frame0_ki...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H:/THESIS/dataset\\val_frames\\kick_07_frame1_ki...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H:/THESIS/dataset\\val_frames\\kick_07_frame2_ki...</td>\n",
       "      <td>kick</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image action\n",
       "0  H:/THESIS/dataset\\val_frames\\kick_02_frame0_ki...   kick\n",
       "1  H:/THESIS/dataset\\val_frames\\kick_02_frame1_ki...   kick\n",
       "2  H:/THESIS/dataset\\val_frames\\kick_07_frame0_ki...   kick\n",
       "3  H:/THESIS/dataset\\val_frames\\kick_07_frame1_ki...   kick\n",
       "4  H:/THESIS/dataset\\val_frames\\kick_07_frame2_ki...   kick"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_image = pd.read_csv(os.path.join(dataset_path, 'val_frames.csv'))\n",
    "val_image.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "silver-peeing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(921, 2)\n"
     ]
    }
   ],
   "source": [
    "print(val_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "chubby-healing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kick', 'punch', 'slap']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_values = list(train_image['action'].unique())\n",
    "action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "center-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_columns(df):\n",
    "    for value in action_values:\n",
    "        df[value] = np.where(df['action'].str.contains(value), 1, 0)\n",
    "    df.drop('action', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "under-veteran",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>kick</th>\n",
       "      <th>punch</th>\n",
       "      <th>slap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H:/THESIS/dataset\\train_frames\\kick_01_frame0_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H:/THESIS/dataset\\train_frames\\kick_01_frame1_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H:/THESIS/dataset\\train_frames\\kick_01_frame2_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H:/THESIS/dataset\\train_frames\\kick_01_frame3_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H:/THESIS/dataset\\train_frames\\kick_02_frame0_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  kick  punch  slap\n",
       "0  H:/THESIS/dataset\\train_frames\\kick_01_frame0_...     1      0     0\n",
       "1  H:/THESIS/dataset\\train_frames\\kick_01_frame1_...     1      0     0\n",
       "2  H:/THESIS/dataset\\train_frames\\kick_01_frame2_...     1      0     0\n",
       "3  H:/THESIS/dataset\\train_frames\\kick_01_frame3_...     1      0     0\n",
       "4  H:/THESIS/dataset\\train_frames\\kick_02_frame0_...     1      0     0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_class_columns(train_image)\n",
    "train_image.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "danish-feelings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>kick</th>\n",
       "      <th>punch</th>\n",
       "      <th>slap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H:/THESIS/dataset\\val_frames\\kick_02_frame0_ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H:/THESIS/dataset\\val_frames\\kick_02_frame1_ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H:/THESIS/dataset\\val_frames\\kick_07_frame0_ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H:/THESIS/dataset\\val_frames\\kick_07_frame1_ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H:/THESIS/dataset\\val_frames\\kick_07_frame2_ki...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  kick  punch  slap\n",
       "0  H:/THESIS/dataset\\val_frames\\kick_02_frame0_ki...     1      0     0\n",
       "1  H:/THESIS/dataset\\val_frames\\kick_02_frame1_ki...     1      0     0\n",
       "2  H:/THESIS/dataset\\val_frames\\kick_07_frame0_ki...     1      0     0\n",
       "3  H:/THESIS/dataset\\val_frames\\kick_07_frame1_ki...     1      0     0\n",
       "4  H:/THESIS/dataset\\val_frames\\kick_07_frame2_ki...     1      0     0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_class_columns(val_image)\n",
    "val_image.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "official-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_array_and_split(image_data):\n",
    "    image_value = []\n",
    "    for i in tqdm(range(image_data.shape[0])):\n",
    "        img = image.load_img(image_data['image'][i], target_size=(224,224,3))\n",
    "        img = image.img_to_array(img)\n",
    "        # normalizing the pixel value\n",
    "        img = img / 255\n",
    "        image_value.append(img)\n",
    "\n",
    "    X = np.array(image_value)\n",
    "    y = image_data\n",
    "    y.drop('image', axis='columns', inplace=True)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "single-moral",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2171/2171 [00:50<00:00, 43.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2171, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = convert_to_array_and_split(train_image)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "recorded-pantyhose",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 921/921 [00:18<00:00, 49.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(921, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val = convert_to_array_and_split(val_image)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "viral-anthony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kick</th>\n",
       "      <th>punch</th>\n",
       "      <th>slap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kick  punch  slap\n",
       "0     1      0     0\n",
       "1     1      0     0\n",
       "2     1      0     0\n",
       "3     1      0     0\n",
       "4     1      0     0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "grateful-croatia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kick</th>\n",
       "      <th>punch</th>\n",
       "      <th>slap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kick  punch  slap\n",
       "0     1      0     0\n",
       "1     1      0     0\n",
       "2     1      0     0\n",
       "3     1      0     0\n",
       "4     1      0     0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-usage",
   "metadata": {},
   "source": [
    "### Processing With Pre-trained VGG16 with imagenet weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "virgin-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "'''This model was trained on a dataset that has 1,000 classes. \n",
    "include_top = False will remove the last layer of this model so that we can tune it as per our need.\n",
    "'''\n",
    "base_model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "personal-elizabeth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2171, 7, 7, 512)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting features for training frames\n",
    "X_train = base_model.predict(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "covered-incident",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(921, 7, 7, 512)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = base_model.predict(X_val)\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fancy-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the training as well as validation frames in single dimension\n",
    "X_train = X_train.reshape(2171, 7*7*512)\n",
    "X_val = X_val.reshape(921, 7*7*512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "mineral-lesson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2171, 25088)\n",
      "(921, 25088)\n"
     ]
    }
   ],
   "source": [
    "# normalizing the pixel values\n",
    "max_pixel = X_train.max()\n",
    "X_train = X_train / max_pixel\n",
    "X_val = X_val / max_pixel\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-bristol",
   "metadata": {},
   "source": [
    "### Deep Neural Network model with 5 layers and3 output neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "boolean-chemical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 26,380,547\n",
      "Trainable params: 26,380,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The input shape will be 25,088\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(25088,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "scenic-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_weight = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "renewable-packaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - 5s 319ms/step - loss: 1.1953 - accuracy: 0.3482 - val_loss: 1.0849 - val_accuracy: 0.3844\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 7s 414ms/step - loss: 1.1157 - accuracy: 0.3676 - val_loss: 1.0657 - val_accuracy: 0.5613\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 6s 377ms/step - loss: 1.0584 - accuracy: 0.4067 - val_loss: 0.9697 - val_accuracy: 0.5603\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 7s 439ms/step - loss: 0.9405 - accuracy: 0.5108 - val_loss: 0.8221 - val_accuracy: 0.6211\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 7s 411ms/step - loss: 0.8461 - accuracy: 0.5758 - val_loss: 0.7219 - val_accuracy: 0.6710\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 7s 395ms/step - loss: 0.7013 - accuracy: 0.6642 - val_loss: 0.5837 - val_accuracy: 0.7557\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 7s 430ms/step - loss: 0.5944 - accuracy: 0.7462 - val_loss: 0.5284 - val_accuracy: 0.7459\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 5s 284ms/step - loss: 0.4924 - accuracy: 0.7872 - val_loss: 0.5322 - val_accuracy: 0.7416\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 7s 439ms/step - loss: 0.3844 - accuracy: 0.8397 - val_loss: 0.3656 - val_accuracy: 0.8556\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 8s 464ms/step - loss: 0.2915 - accuracy: 0.8885 - val_loss: 0.3211 - val_accuracy: 0.8882\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 4s 260ms/step - loss: 0.2096 - accuracy: 0.9240 - val_loss: 0.3593 - val_accuracy: 0.9034\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 5s 288ms/step - loss: 0.2375 - accuracy: 0.9148 - val_loss: 0.3428 - val_accuracy: 0.8979\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 8s 460ms/step - loss: 0.1949 - accuracy: 0.9222 - val_loss: 0.2897 - val_accuracy: 0.9099\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 5s 268ms/step - loss: 0.1270 - accuracy: 0.9572 - val_loss: 0.3405 - val_accuracy: 0.9164\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 5s 270ms/step - loss: 0.1277 - accuracy: 0.9558 - val_loss: 0.3713 - val_accuracy: 0.9088\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 5s 282ms/step - loss: 0.1053 - accuracy: 0.9650 - val_loss: 0.3271 - val_accuracy: 0.9240\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 5s 283ms/step - loss: 0.1013 - accuracy: 0.9659 - val_loss: 0.3384 - val_accuracy: 0.9273\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 5s 281ms/step - loss: 0.0768 - accuracy: 0.9737 - val_loss: 0.3481 - val_accuracy: 0.9240\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 4s 254ms/step - loss: 0.0912 - accuracy: 0.9668 - val_loss: 0.3551 - val_accuracy: 0.9283\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 5s 279ms/step - loss: 0.0804 - accuracy: 0.9696 - val_loss: 0.3349 - val_accuracy: 0.9218\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 5s 288ms/step - loss: 0.0780 - accuracy: 0.9765 - val_loss: 0.3009 - val_accuracy: 0.9294\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 5s 278ms/step - loss: 0.0879 - accuracy: 0.9705 - val_loss: 0.3595 - val_accuracy: 0.9197\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 5s 285ms/step - loss: 0.0874 - accuracy: 0.9719 - val_loss: 0.3372 - val_accuracy: 0.9327\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 5s 276ms/step - loss: 0.0789 - accuracy: 0.9728 - val_loss: 0.3019 - val_accuracy: 0.9273\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 4s 241ms/step - loss: 0.0961 - accuracy: 0.9673 - val_loss: 0.3816 - val_accuracy: 0.9142\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 4s 230ms/step - loss: 0.0915 - accuracy: 0.9691 - val_loss: 0.3569 - val_accuracy: 0.9273\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 5s 267ms/step - loss: 0.0810 - accuracy: 0.9714 - val_loss: 0.3229 - val_accuracy: 0.9262\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 4s 251ms/step - loss: 0.0987 - accuracy: 0.9655 - val_loss: 0.3556 - val_accuracy: 0.9207\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 7s 400ms/step - loss: 0.0928 - accuracy: 0.9678 - val_loss: 0.2716 - val_accuracy: 0.9283\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 5s 286ms/step - loss: 0.0684 - accuracy: 0.9751 - val_loss: 0.3348 - val_accuracy: 0.9240\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 5s 293ms/step - loss: 0.0762 - accuracy: 0.9756 - val_loss: 0.3770 - val_accuracy: 0.9273\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 4s 251ms/step - loss: 0.0636 - accuracy: 0.9820 - val_loss: 0.3408 - val_accuracy: 0.9283\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 5s 282ms/step - loss: 0.0760 - accuracy: 0.9760 - val_loss: 0.3504 - val_accuracy: 0.9305\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 5s 278ms/step - loss: 0.0847 - accuracy: 0.9714 - val_loss: 0.3305 - val_accuracy: 0.9240\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 5s 294ms/step - loss: 0.0678 - accuracy: 0.9770 - val_loss: 0.3051 - val_accuracy: 0.9349\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 5s 277ms/step - loss: 0.0555 - accuracy: 0.9816 - val_loss: 0.4003 - val_accuracy: 0.9338\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 5s 299ms/step - loss: 0.0686 - accuracy: 0.9737 - val_loss: 0.3298 - val_accuracy: 0.9273\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 5s 278ms/step - loss: 0.0860 - accuracy: 0.9664 - val_loss: 0.3110 - val_accuracy: 0.9273\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 5s 311ms/step - loss: 0.0939 - accuracy: 0.9691 - val_loss: 0.4329 - val_accuracy: 0.9316\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 5s 275ms/step - loss: 0.0821 - accuracy: 0.9710 - val_loss: 0.3424 - val_accuracy: 0.9327\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 5s 279ms/step - loss: 0.0576 - accuracy: 0.9797 - val_loss: 0.3909 - val_accuracy: 0.9338\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 5s 287ms/step - loss: 0.0533 - accuracy: 0.9834 - val_loss: 0.3466 - val_accuracy: 0.9262\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 5s 294ms/step - loss: 0.0412 - accuracy: 0.9862 - val_loss: 0.3924 - val_accuracy: 0.9327\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 4s 262ms/step - loss: 0.0413 - accuracy: 0.9816 - val_loss: 0.4424 - val_accuracy: 0.9349\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 5s 272ms/step - loss: 0.0458 - accuracy: 0.9839 - val_loss: 0.4406 - val_accuracy: 0.9283\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 4s 261ms/step - loss: 0.0588 - accuracy: 0.9820 - val_loss: 0.3896 - val_accuracy: 0.9273\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 5s 278ms/step - loss: 0.0645 - accuracy: 0.9770 - val_loss: 0.4232 - val_accuracy: 0.9262\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 5s 272ms/step - loss: 0.0643 - accuracy: 0.9774 - val_loss: 0.4254 - val_accuracy: 0.9229\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 5s 279ms/step - loss: 0.0536 - accuracy: 0.9843 - val_loss: 0.4765 - val_accuracy: 0.9294\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 5s 269ms/step - loss: 0.0661 - accuracy: 0.9784 - val_loss: 0.3986 - val_accuracy: 0.9283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bd1df50940>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[mcp_weight], batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-refund",
   "metadata": {},
   "source": [
    "### Applying Built model  on Test data and calculate accuracy with VGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "internal-growing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 26,380,547\n",
      "Trainable params: 26,380,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"weight.hdf5\")\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "comic-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 123/123 [02:32<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats as s\n",
    "predict = []\n",
    "actual = []\n",
    "if not os.path.exists(test_frames_path):\n",
    "    os.makedirs(test_frames_path)\n",
    "\n",
    "for i in tqdm(range(test.shape[0])):\n",
    "    video_file = test['Video_url'][i]\n",
    "    action = test['action'][i]\n",
    "    video_name_list = video_file.split('/')[-1].split('.')\n",
    "    video_name_list = video_name_list[:-1]\n",
    "    video_name = \"\"\n",
    "    for n in video_name_list:\n",
    "        video_name += n\n",
    "    # capturing the video from the given path\n",
    "    capture = cv2.VideoCapture(video_file) \n",
    "    #frame rate\n",
    "    frame_rate = capture.get(5)\n",
    "    count = 0\n",
    "    files = glob(test_frames_path + '/*')\n",
    "    #removing all files from folder\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    while(capture.isOpened()):\n",
    "        #current frame number\n",
    "        frame_id = capture.get(1) \n",
    "        read_correctly, frame = capture.read()\n",
    "        if not read_correctly:\n",
    "            break\n",
    "        if (frame_id % math.floor(frame_rate) == 0):\n",
    "            # storing the frames in a new folder named train_1\n",
    "            filename = test_frames_path + \"/\" + video_name + \"_frame{}_\".format(count) + action +\".jpg\"\n",
    "            count += 1\n",
    "            cv2.imwrite(filename, frame)\n",
    "    capture.release()\n",
    "    \n",
    "    # reading all the frames from temp folder\n",
    "    images = glob(test_frames_path + '/*.jpg')\n",
    "    prediction_images = []\n",
    "    for i in range(len(images)):\n",
    "        img = image.load_img(images[i], target_size=(224,224,3))\n",
    "        img = image.img_to_array(img)\n",
    "        img = img / 255\n",
    "        prediction_images.append(img)\n",
    "        \n",
    "    # converting all the frames for a test video into numpy array\n",
    "    prediction_images = np.array(prediction_images)\n",
    "    # extracting features using pre-trained model\n",
    "    prediction_images = base_model.predict(prediction_images)\n",
    "    # converting features in one dimensional array\n",
    "    prediction_images = prediction_images.reshape(prediction_images.shape[0], 7*7*512)\n",
    "    # predicting tags for each array\n",
    "    prediction = np.argmax(model.predict(prediction_images), axis=-1)\n",
    "    # appending the mode of predictions in predict list to assign the tag to the video\n",
    "    predict.append(y_train.columns.values[s.mode(prediction)[0][0]])\n",
    "    # appending the actual tag of the video\n",
    "    actual.append(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "metropolitan-freeze",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kick'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.columns.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "heavy-timothy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.6829268292683"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(predict, actual)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-things",
   "metadata": {},
   "source": [
    "### Processing With Pre-trained InceptionV3 with imagenet weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "further-premises",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 43s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "inception_model = InceptionV3(include_top = False, weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "amino-alcohol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2171, 5, 5, 2048)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting features for training frames\n",
    "X_train = inception_model.predict(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dental-cooper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(921, 5, 5, 2048)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = inception_model.predict(X_val)\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "assisted-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the training as well as validation frames in single dimension\n",
    "X_train = X_train.reshape(2171, 5*5*2048)\n",
    "X_val = X_val.reshape(921, 5*5*2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cloudy-drilling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2171, 51200)\n",
      "(921, 51200)\n"
     ]
    }
   ],
   "source": [
    "# normalizing the pixel values\n",
    "max_pixel = X_train.max()\n",
    "X_train = X_train / max_pixel\n",
    "X_val = X_val / max_pixel\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "separated-giving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 1024)              52429824  \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 53,119,235\n",
      "Trainable params: 53,119,235\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The input shape will be 51200\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(51200,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "antique-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_weight = ModelCheckpoint('weight1.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "retained-association",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - 8s 455ms/step - loss: 0.0459 - accuracy: 0.9848 - val_loss: 0.6965 - val_accuracy: 0.9066\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 8s 469ms/step - loss: 0.0666 - accuracy: 0.9797 - val_loss: 0.6176 - val_accuracy: 0.8990\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 8s 458ms/step - loss: 0.0527 - accuracy: 0.9843 - val_loss: 0.6065 - val_accuracy: 0.8979\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 8s 452ms/step - loss: 0.0616 - accuracy: 0.9816 - val_loss: 0.6853 - val_accuracy: 0.9034\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 7s 441ms/step - loss: 0.0488 - accuracy: 0.9857 - val_loss: 0.7090 - val_accuracy: 0.9034\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 7s 440ms/step - loss: 0.0443 - accuracy: 0.9834 - val_loss: 0.5595 - val_accuracy: 0.9034\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 7s 397ms/step - loss: 0.0320 - accuracy: 0.9889 - val_loss: 0.6417 - val_accuracy: 0.9066\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 7s 419ms/step - loss: 0.0348 - accuracy: 0.9912 - val_loss: 0.6392 - val_accuracy: 0.9088\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 7s 431ms/step - loss: 0.0430 - accuracy: 0.9857 - val_loss: 0.6208 - val_accuracy: 0.9099\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 7s 434ms/step - loss: 0.0414 - accuracy: 0.9843 - val_loss: 0.6048 - val_accuracy: 0.9066\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 7s 416ms/step - loss: 0.0248 - accuracy: 0.9926 - val_loss: 0.6048 - val_accuracy: 0.9121\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 7s 441ms/step - loss: 0.0236 - accuracy: 0.9903 - val_loss: 0.6669 - val_accuracy: 0.9099\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 7s 423ms/step - loss: 0.0430 - accuracy: 0.9866 - val_loss: 0.5738 - val_accuracy: 0.9055\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 7s 433ms/step - loss: 0.0354 - accuracy: 0.9899 - val_loss: 0.5567 - val_accuracy: 0.9110\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 7s 440ms/step - loss: 0.0192 - accuracy: 0.9926 - val_loss: 0.6670 - val_accuracy: 0.9034\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 8s 458ms/step - loss: 0.0297 - accuracy: 0.9917 - val_loss: 0.6952 - val_accuracy: 0.9077\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 8s 444ms/step - loss: 0.0290 - accuracy: 0.9912 - val_loss: 0.6395 - val_accuracy: 0.9088\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 7s 435ms/step - loss: 0.0224 - accuracy: 0.9940 - val_loss: 0.6765 - val_accuracy: 0.9077\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 7s 434ms/step - loss: 0.0260 - accuracy: 0.9908 - val_loss: 0.7947 - val_accuracy: 0.9066\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 7s 421ms/step - loss: 0.0375 - accuracy: 0.9903 - val_loss: 0.6231 - val_accuracy: 0.9142\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 7s 429ms/step - loss: 0.0146 - accuracy: 0.9959 - val_loss: 0.7434 - val_accuracy: 0.9045\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 7s 440ms/step - loss: 0.0190 - accuracy: 0.9931 - val_loss: 0.8017 - val_accuracy: 0.9099\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 8s 448ms/step - loss: 0.0252 - accuracy: 0.9926 - val_loss: 0.7544 - val_accuracy: 0.9034\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 7s 422ms/step - loss: 0.0281 - accuracy: 0.9903 - val_loss: 0.7231 - val_accuracy: 0.9034\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 7s 420ms/step - loss: 0.0175 - accuracy: 0.9945 - val_loss: 0.8353 - val_accuracy: 0.9066\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 7s 417ms/step - loss: 0.0243 - accuracy: 0.9903 - val_loss: 0.7618 - val_accuracy: 0.9077\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 7s 414ms/step - loss: 0.0215 - accuracy: 0.9912 - val_loss: 0.7905 - val_accuracy: 0.9077\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 7s 426ms/step - loss: 0.0303 - accuracy: 0.9889 - val_loss: 0.8579 - val_accuracy: 0.9001\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 7s 411ms/step - loss: 0.0395 - accuracy: 0.9876 - val_loss: 0.5951 - val_accuracy: 0.9055\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 7s 407ms/step - loss: 0.0492 - accuracy: 0.9839 - val_loss: 0.6601 - val_accuracy: 0.9077\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 7s 405ms/step - loss: 0.0321 - accuracy: 0.9894 - val_loss: 0.6895 - val_accuracy: 0.9045\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 7s 392ms/step - loss: 0.0306 - accuracy: 0.9908 - val_loss: 0.6341 - val_accuracy: 0.9110\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 7s 416ms/step - loss: 0.0372 - accuracy: 0.9899 - val_loss: 0.8041 - val_accuracy: 0.9088\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 7s 399ms/step - loss: 0.0426 - accuracy: 0.9853 - val_loss: 0.6594 - val_accuracy: 0.9001\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 7s 391ms/step - loss: 0.0388 - accuracy: 0.9880 - val_loss: 0.7272 - val_accuracy: 0.9045\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 7s 413ms/step - loss: 0.0315 - accuracy: 0.9908 - val_loss: 0.5809 - val_accuracy: 0.9055\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 7s 418ms/step - loss: 0.0269 - accuracy: 0.9903 - val_loss: 0.6879 - val_accuracy: 0.9045\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 8s 451ms/step - loss: 0.0255 - accuracy: 0.9908 - val_loss: 0.7870 - val_accuracy: 0.9034\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 7s 433ms/step - loss: 0.0291 - accuracy: 0.9922 - val_loss: 0.7550 - val_accuracy: 0.9012\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 8s 446ms/step - loss: 0.0247 - accuracy: 0.9912 - val_loss: 0.7183 - val_accuracy: 0.9001\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 7s 433ms/step - loss: 0.0117 - accuracy: 0.9963 - val_loss: 0.7923 - val_accuracy: 0.8990\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 7s 423ms/step - loss: 0.0208 - accuracy: 0.9931 - val_loss: 0.7579 - val_accuracy: 0.8969\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 6s 381ms/step - loss: 0.0505 - accuracy: 0.9862 - val_loss: 0.7426 - val_accuracy: 0.9001\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 7s 422ms/step - loss: 0.0303 - accuracy: 0.9912 - val_loss: 0.5802 - val_accuracy: 0.9055\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 7s 424ms/step - loss: 0.0251 - accuracy: 0.9917 - val_loss: 0.6896 - val_accuracy: 0.9023\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 7s 434ms/step - loss: 0.0225 - accuracy: 0.9926 - val_loss: 0.7230 - val_accuracy: 0.9045\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 7s 423ms/step - loss: 0.0130 - accuracy: 0.9968 - val_loss: 0.7963 - val_accuracy: 0.8990\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 7s 436ms/step - loss: 0.0074 - accuracy: 0.9982 - val_loss: 0.8845 - val_accuracy: 0.9023\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 7s 439ms/step - loss: 0.0281 - accuracy: 0.9903 - val_loss: 0.8095 - val_accuracy: 0.9012\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 7s 424ms/step - loss: 0.0267 - accuracy: 0.9917 - val_loss: 0.7617 - val_accuracy: 0.9045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bd0fe58130>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[mcp_weight], batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-trinity",
   "metadata": {},
   "source": [
    "### Applying Built model  on Test data and calculate accuracy with InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "material-chapel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 123/123 [01:50<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats as s\n",
    "predict = []\n",
    "actual = []\n",
    "if not os.path.exists(test_frames_path):\n",
    "    os.makedirs(test_frames_path)\n",
    "\n",
    "for i in tqdm(range(test.shape[0])):\n",
    "    video_file = test['Video_url'][i]\n",
    "    action = test['action'][i]\n",
    "    video_name_list = video_file.split('/')[-1].split('.')\n",
    "    video_name_list = video_name_list[:-1]\n",
    "    video_name = \"\"\n",
    "    for n in video_name_list:\n",
    "        video_name += n\n",
    "    # capturing the video from the given path\n",
    "    capture = cv2.VideoCapture(video_file) \n",
    "    #frame rate\n",
    "    frame_rate = capture.get(5)\n",
    "    count = 0\n",
    "    files = glob(test_frames_path + '/*')\n",
    "    #removing all files from folder\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    while(capture.isOpened()):\n",
    "        #current frame number\n",
    "        frame_id = capture.get(1) \n",
    "        read_correctly, frame = capture.read()\n",
    "        if not read_correctly:\n",
    "            break\n",
    "        if (frame_id % math.floor(frame_rate) == 0):\n",
    "            # storing the frames in a new folder named train_1\n",
    "            filename = test_frames_path + \"/\" + video_name + \"_frame{}_\".format(count) + action +\".jpg\"\n",
    "            count += 1\n",
    "            cv2.imwrite(filename, frame)\n",
    "    capture.release()\n",
    "    \n",
    "    # reading all the frames from temp folder\n",
    "    images = glob(test_frames_path + '/*.jpg')\n",
    "    prediction_images = []\n",
    "    for i in range(len(images)):\n",
    "        img = image.load_img(images[i], target_size=(224,224,3))\n",
    "        img = image.img_to_array(img)\n",
    "        img = img / 255\n",
    "        prediction_images.append(img)\n",
    "        \n",
    "    # converting all the frames for a test video into numpy array\n",
    "    prediction_images = np.array(prediction_images)\n",
    "    # extracting features using pre-trained model\n",
    "    prediction_images = inception_model.predict(prediction_images)\n",
    "    # converting features in one dimensional array\n",
    "    prediction_images = prediction_images.reshape(prediction_images.shape[0], 5*5*2048)\n",
    "    # predicting tags for each array\n",
    "    prediction = np.argmax(model.predict(prediction_images), axis=-1)\n",
    "    # appending the mode of predictions in predict list to assign the tag to the video\n",
    "    predict.append(y_train.columns.values[s.mode(prediction)[0][0]])\n",
    "    # appending the actual tag of the video\n",
    "    actual.append(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "anticipated-blake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.869918699187"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(predict, actual)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-mercy",
   "metadata": {},
   "source": [
    "### Processing With Pre-trained ResNet50 with imagenet weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "grand-belief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 45s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "resnet_model = ResNet50(include_top=False, weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "demographic-athletics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2171, 7, 7, 2048)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = resnet_model.predict(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "conservative-fight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(921, 7, 7, 2048)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = resnet_model.predict(X_val)\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "opening-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(2171, 7*7*2048)\n",
    "X_val = X_val.reshape(921, 7*7*2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "empty-aaron",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2171, 100352)\n",
      "(921, 100352)\n"
     ]
    }
   ],
   "source": [
    "max_pixel = X_train.max()\n",
    "X_train = X_train / max_pixel\n",
    "X_val = X_val / max_pixel\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "skilled-secondary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 1024)              102761472 \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 103,450,883\n",
      "Trainable params: 103,450,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# The input shape will be 100352\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(100352,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "following-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_weight = ModelCheckpoint('weight2.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "outstanding-match",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - 23s 1s/step - loss: 1.2903 - accuracy: 0.3459 - val_loss: 1.1094 - val_accuracy: 0.3181\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 27s 2s/step - loss: 1.1570 - accuracy: 0.3399 - val_loss: 1.0996 - val_accuracy: 0.3181\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 26s 2s/step - loss: 1.1123 - accuracy: 0.3496 - val_loss: 1.0926 - val_accuracy: 0.3865\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 1.1080 - accuracy: 0.3542 - val_loss: 1.0932 - val_accuracy: 0.3865\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 14s 820ms/step - loss: 1.1026 - accuracy: 0.3671 - val_loss: 1.0942 - val_accuracy: 0.3865\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 14s 800ms/step - loss: 1.0995 - accuracy: 0.3630 - val_loss: 1.0952 - val_accuracy: 0.3865\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 14s 797ms/step - loss: 1.0960 - accuracy: 0.3657 - val_loss: 1.0947 - val_accuracy: 0.3865\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 14s 814ms/step - loss: 1.0946 - accuracy: 0.3754 - val_loss: 1.0934 - val_accuracy: 0.3865\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 14s 809ms/step - loss: 1.0932 - accuracy: 0.3736 - val_loss: 1.0930 - val_accuracy: 0.3865\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 26s 2s/step - loss: 1.0921 - accuracy: 0.3694 - val_loss: 1.0923 - val_accuracy: 0.3865\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 14s 818ms/step - loss: 1.0914 - accuracy: 0.3791 - val_loss: 1.0924 - val_accuracy: 0.3865\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 25s 1s/step - loss: 1.0887 - accuracy: 0.3791 - val_loss: 1.0863 - val_accuracy: 0.3865\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 26s 2s/step - loss: 1.0829 - accuracy: 0.3851 - val_loss: 1.0770 - val_accuracy: 0.3865\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 26s 2s/step - loss: 1.0782 - accuracy: 0.3805 - val_loss: 1.0694 - val_accuracy: 0.3865\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 25s 1s/step - loss: 1.0693 - accuracy: 0.3777 - val_loss: 1.0538 - val_accuracy: 0.3865\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 13s 787ms/step - loss: 1.0650 - accuracy: 0.3934 - val_loss: 1.0684 - val_accuracy: 0.4235\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 25s 1s/step - loss: 1.0653 - accuracy: 0.3795 - val_loss: 1.0485 - val_accuracy: 0.4311\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 26s 2s/step - loss: 1.0428 - accuracy: 0.4275 - val_loss: 1.0320 - val_accuracy: 0.4072\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 24s 1s/step - loss: 1.0336 - accuracy: 0.4344 - val_loss: 1.0185 - val_accuracy: 0.4864\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 27s 2s/step - loss: 1.0167 - accuracy: 0.4500 - val_loss: 1.0085 - val_accuracy: 0.4929\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 24s 1s/step - loss: 1.0164 - accuracy: 0.4505 - val_loss: 0.9742 - val_accuracy: 0.5309\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 25s 1s/step - loss: 1.0012 - accuracy: 0.4477 - val_loss: 0.9646 - val_accuracy: 0.5461\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 25s 1s/step - loss: 0.9848 - accuracy: 0.4666 - val_loss: 0.9632 - val_accuracy: 0.5375\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 13s 775ms/step - loss: 1.0069 - accuracy: 0.4509 - val_loss: 0.9723 - val_accuracy: 0.5375\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.9794 - accuracy: 0.4717 - val_loss: 0.9362 - val_accuracy: 0.5494\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 14s 797ms/step - loss: 0.9822 - accuracy: 0.4823 - val_loss: 0.9426 - val_accuracy: 0.5635\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 13s 781ms/step - loss: 0.9750 - accuracy: 0.4823 - val_loss: 0.9714 - val_accuracy: 0.5114\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 13s 777ms/step - loss: 1.0050 - accuracy: 0.4551 - val_loss: 1.0022 - val_accuracy: 0.4571\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 13s 785ms/step - loss: 0.9845 - accuracy: 0.4565 - val_loss: 0.9538 - val_accuracy: 0.5342\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 13s 769ms/step - loss: 0.9803 - accuracy: 0.4560 - val_loss: 0.9786 - val_accuracy: 0.5461\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 25s 1s/step - loss: 0.9555 - accuracy: 0.4818 - val_loss: 0.9327 - val_accuracy: 0.5483\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 27s 2s/step - loss: 0.9520 - accuracy: 0.4712 - val_loss: 0.8791 - val_accuracy: 0.5722\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 14s 830ms/step - loss: 0.9576 - accuracy: 0.4698 - val_loss: 0.9499 - val_accuracy: 0.4832\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 14s 846ms/step - loss: 0.9546 - accuracy: 0.4698 - val_loss: 0.8910 - val_accuracy: 0.5592\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.9551 - accuracy: 0.4657 - val_loss: 0.8664 - val_accuracy: 0.5928\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 12s 716ms/step - loss: 0.9291 - accuracy: 0.5030 - val_loss: 0.9117 - val_accuracy: 0.5212\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 13s 749ms/step - loss: 0.9322 - accuracy: 0.4979 - val_loss: 0.8834 - val_accuracy: 0.5798\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.9073 - accuracy: 0.5025 - val_loss: 0.8418 - val_accuracy: 0.6048\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.8864 - accuracy: 0.5269 - val_loss: 0.8241 - val_accuracy: 0.6221\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 26s 2s/step - loss: 0.8776 - accuracy: 0.5334 - val_loss: 0.8148 - val_accuracy: 0.6221\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 13s 767ms/step - loss: 0.8891 - accuracy: 0.5292 - val_loss: 0.8364 - val_accuracy: 0.5993\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 14s 800ms/step - loss: 0.9187 - accuracy: 0.4961 - val_loss: 0.8646 - val_accuracy: 0.6308\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 24s 1s/step - loss: 0.9223 - accuracy: 0.4952 - val_loss: 0.7988 - val_accuracy: 0.6450\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 14s 834ms/step - loss: 0.8483 - accuracy: 0.5352 - val_loss: 0.8182 - val_accuracy: 0.6526\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 27s 2s/step - loss: 0.8441 - accuracy: 0.5532 - val_loss: 0.7837 - val_accuracy: 0.6450\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 13s 768ms/step - loss: 0.8720 - accuracy: 0.5288 - val_loss: 0.8169 - val_accuracy: 0.6102\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 13s 753ms/step - loss: 0.8927 - accuracy: 0.5352 - val_loss: 0.8307 - val_accuracy: 0.6276\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 13s 758ms/step - loss: 0.8685 - accuracy: 0.5394 - val_loss: 0.8272 - val_accuracy: 0.5787\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 23s 1s/step - loss: 0.8720 - accuracy: 0.5219 - val_loss: 0.7246 - val_accuracy: 0.6634\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 13s 792ms/step - loss: 0.8361 - accuracy: 0.5495 - val_loss: 0.7820 - val_accuracy: 0.6645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bd0fef4910>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[mcp_weight], batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-worry",
   "metadata": {},
   "source": [
    "### Applying Built model  on Test data and calculate accuracy with Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "anonymous-trinity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 123/123 [02:15<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats as s\n",
    "predict = []\n",
    "actual = []\n",
    "if not os.path.exists(test_frames_path):\n",
    "    os.makedirs(test_frames_path)\n",
    "\n",
    "for i in tqdm(range(test.shape[0])):\n",
    "    video_file = test['Video_url'][i]\n",
    "    action = test['action'][i]\n",
    "    video_name_list = video_file.split('/')[-1].split('.')\n",
    "    video_name_list = video_name_list[:-1]\n",
    "    video_name = \"\"\n",
    "    for n in video_name_list:\n",
    "        video_name += n\n",
    "    # capturing the video from the given path\n",
    "    capture = cv2.VideoCapture(video_file) \n",
    "    #frame rate\n",
    "    frame_rate = capture.get(5)\n",
    "    count = 0\n",
    "    files = glob(test_frames_path + '/*')\n",
    "    #removing all files from folder\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    while(capture.isOpened()):\n",
    "        #current frame number\n",
    "        frame_id = capture.get(1) \n",
    "        read_correctly, frame = capture.read()\n",
    "        if not read_correctly:\n",
    "            break\n",
    "        if (frame_id % math.floor(frame_rate) == 0):\n",
    "            # storing the frames in a new folder named train_1\n",
    "            filename = test_frames_path + \"/\" + video_name + \"_frame{}_\".format(count) + action +\".jpg\"\n",
    "            count += 1\n",
    "            cv2.imwrite(filename, frame)\n",
    "    capture.release()\n",
    "    \n",
    "    # reading all the frames from temp folder\n",
    "    images = glob(test_frames_path + '/*.jpg')\n",
    "    prediction_images = []\n",
    "    for i in range(len(images)):\n",
    "        img = image.load_img(images[i], target_size=(224,224,3))\n",
    "        img = image.img_to_array(img)\n",
    "        img = img / 255\n",
    "        prediction_images.append(img)\n",
    "        \n",
    "    # converting all the frames for a test video into numpy array\n",
    "    prediction_images = np.array(prediction_images)\n",
    "    # extracting features using pre-trained model\n",
    "    prediction_images = resnet_model.predict(prediction_images)\n",
    "    # converting features in one dimensional array\n",
    "    prediction_images = prediction_images.reshape(prediction_images.shape[0], 7*7*2048)\n",
    "    # predicting tags for each array\n",
    "    prediction = np.argmax(model.predict(prediction_images), axis=-1)\n",
    "    # appending the mode of predictions in predict list to assign the tag to the video\n",
    "    predict.append(y_train.columns.values[s.mode(prediction)[0][0]])\n",
    "    # appending the actual tag of the video\n",
    "    actual.append(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "proprietary-coach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.66666666666666"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predict, actual)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-aircraft",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
